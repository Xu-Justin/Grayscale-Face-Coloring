{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fcfb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True  # Biar ga banyak warning!\n",
    "logging.getLogger('absl').disabled = True        # Biar ga banyak warning!\n",
    "\n",
    "import config\n",
    "import os, cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a0732",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be90798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(keras.utils.Sequence):\n",
    "    augmentation = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.RandomFlip(mode='horizontal'),\n",
    "            keras.layers.RandomZoom(height_factor=(-0.2,0.2)),\n",
    "            keras.layers.RandomTranslation(height_factor=(-0.2,0.2), width_factor=(-0.2,0.2)),\n",
    "            keras.layers.RandomRotation(factor = (0.2))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def __init__(self, path, batch_size, input_shape, output_shape):\n",
    "        self.path = path\n",
    "        self.file_names = os.listdir(path)\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil( len(self.file_names) / self.batch_size ))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        def load_image(file_name):\n",
    "            image = cv2.imread( os.path.join(self.path, file_name) )\n",
    "            return image\n",
    "        \n",
    "        def bgr2gray(image):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            return image\n",
    "        \n",
    "        def resize_image(image, width, height):\n",
    "            image = cv2.resize( image, (width, height) )\n",
    "            return image\n",
    "        \n",
    "        filenames = self.file_names[index*self.batch_size:min((index+1)*self.batch_size, len(self.file_names))]\n",
    "        batch_color = np.array([load_image(filename) for filename in filenames])\n",
    "        batch_color = self.augmentation(batch_color).numpy()\n",
    "        batch_grayscale = [bgr2gray(image) for image in batch_color]\n",
    "        \n",
    "        batch_color = [resize_image(image, self.output_shape[1], self.output_shape[0]) for image in batch_color]\n",
    "        batch_grayscale = [resize_image(image, self.input_shape[1], self.input_shape[0]) for image in batch_grayscale]\n",
    "        \n",
    "        batch_color = (np.asarray(batch_color, 'float32') / 127.5 ) - 1\n",
    "        batch_grayscale = (np.asarray(batch_grayscale, 'float32') / 127.5 ) -1\n",
    "        \n",
    "        assert(batch_grayscale[0].shape == self.input_shape)\n",
    "        assert(batch_color[0].shape == self.output_shape)\n",
    "        \n",
    "        return batch_grayscale, batch_color\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dceec58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = generator(config.dir_train, batch_size, config.input_shape, config.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47896b8",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f53b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_generator(input_shape):\n",
    "    init_weight = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        \n",
    "    def encoder(in_layer, k, batch_normalization=True):\n",
    "        net = keras.layers.Conv2D(k, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init_weight)(in_layer)\n",
    "        if(batch_normalization):\n",
    "            net = keras.layers.BatchNormalization()(net, training=True)\n",
    "        net = keras.layers.LeakyReLU(alpha=0.2)(net)\n",
    "        return net\n",
    "    \n",
    "    def decoder(in_layer, skip_layer, k, batch_normalization=True, dropout=True):\n",
    "        net = keras.layers.Conv2DTranspose(k, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init_weight)(in_layer)\n",
    "        if(batch_normalization):\n",
    "            net = keras.layers.BatchNormalization()(net, training=True)\n",
    "        if(dropout):\n",
    "            net = keras.layers.Dropout(0.5)(net, training=True)\n",
    "        net = keras.layers.Concatenate()([net, skip_layer])\n",
    "        net = keras.layers.ReLU()(net)\n",
    "        return net\n",
    "        \n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "    input_layer = keras.layers.Reshape((input_shape[0], input_shape[1], 1))(input_layer)\n",
    "    \n",
    "    encoder_1 = encoder(input_layer, 64, batch_normalization=False)\n",
    "    encoder_2 = encoder(encoder_1, 128)\n",
    "    encoder_3 = encoder(encoder_2, 256)\n",
    "    encoder_4 = encoder(encoder_3, 512)\n",
    "    encoder_5 = encoder(encoder_4, 512)\n",
    "    encoder_6 = encoder(encoder_5, 512)\n",
    "    encoder_7 = encoder(encoder_6, 512)\n",
    "    encoder_8 = encoder(encoder_7, 512)\n",
    "    \n",
    "    decoder_1 = decoder(encoder_8, encoder_7, 512)\n",
    "    decoder_2 = decoder(decoder_1, encoder_6, 512)\n",
    "    decoder_3 = decoder(decoder_2, encoder_5, 512)\n",
    "    decoder_4 = decoder(decoder_3, encoder_4, 512, dropout=False)\n",
    "    decoder_5 = decoder(decoder_4, encoder_3, 256, dropout=False)\n",
    "    decoder_6 = decoder(decoder_5, encoder_2, 128, dropout=False)\n",
    "    decoder_7 = decoder(decoder_6, encoder_1, 64, dropout=False)\n",
    "    \n",
    "    output_layer = keras.layers.Conv2DTranspose(3, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init_weight)(decoder_7)\n",
    "    output_layer = keras.layers.Activation(keras.activations.tanh)(output_layer)\n",
    "    model=keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "def create_model_discriminator(input_shape, output_shape):\n",
    "    init_weight = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        \n",
    "    def encoder(in_layer, k, batch_normalization=True):\n",
    "        net = keras.layers.Conv2D(k, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init_weight)(in_layer)\n",
    "        if(batch_normalization):\n",
    "            net = keras.layers.BatchNormalization()(net, training=True)\n",
    "        net = keras.layers.LeakyReLU(alpha=0.2)(net)\n",
    "        return net\n",
    "    \n",
    "    input_layer_1 = keras.Input(shape=input_shape)\n",
    "    input_layer_1 = keras.layers.Reshape((input_shape[0], input_shape[1], 1))(input_layer_1)\n",
    "    \n",
    "    input_layer_2 = keras.Input(shape=output_shape)\n",
    "    \n",
    "    net = keras.layers.Concatenate()([input_layer_1, input_layer_2])\n",
    "    net = encoder(net, 64, batch_normalization=False)\n",
    "    net = encoder(net, 128)\n",
    "    net = encoder(net, 256)\n",
    "    net = encoder(net, 512)\n",
    "    \n",
    "    output_layer = keras.layers.Conv2D(1, kernel_size=(4,4), padding='same', kernel_initializer=init_weight)(net)\n",
    "    output_layer = keras.layers.Activation(keras.activations.sigmoid)(output_layer)\n",
    "    model=keras.Model(inputs=[input_layer_1, input_layer_2],outputs=[output_layer])\n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        loss_weights = [0.5],\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_model_gan(model_generator, model_discriminator, input_shape):\n",
    "    for layer in model_discriminator.layers:\n",
    "        if(not isinstance(layer, keras.layers.BatchNormalization)):\n",
    "            layer.trainable = False\n",
    "        \n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "    generator_output = model_generator(input_layer)\n",
    "    discriminator_output = model_discriminator([input_layer, generator_output])\n",
    "    \n",
    "    model=keras.Model(inputs=[input_layer],outputs=[generator_output, discriminator_output])\n",
    "    model.compile(\n",
    "        loss = ['mae', 'binary_crossentropy'],\n",
    "        loss_weights = [100, 1],\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "def load_model(path):\n",
    "    print('Loading model from %s'%(path), end='\\r')\n",
    "    model = keras.models.load_model(path)\n",
    "    print('Successfully loaded model from %s'%(path))\n",
    "    return model\n",
    "\n",
    "def save_model(path, model):\n",
    "    print('Saving model to %s'%(path), end='\\r')\n",
    "    model.save(path)\n",
    "    print('Successfully saved model to %s'%(path))\n",
    "\n",
    "def save_plot(path, model):\n",
    "    print('Saving model plot to %s'%(path), end='\\r')\n",
    "    keras.utils.plot_model(model, path, show_shapes=True, expand_nested=True)\n",
    "    print('Successfully saved model plot to %s'%(path))\n",
    "\n",
    "def save_summary(path, model):\n",
    "    print('Saving model summary to %s'%(path), end='\\r')\n",
    "    with open(path, 'w') as f:\n",
    "        model.summary(print_fn=lambda x:f.write(x + '\\n'), expand_nested=True)\n",
    "    print('Successfully saved model summary to %s'%(path))\n",
    "    \n",
    "# =================================================================\n",
    "\n",
    "def load_model_gan(path):\n",
    "    model_generator = load_model(os.path.join(path, 'generator'))\n",
    "    model_discriminator = load_model(os.path.join(path, 'discriminator'))\n",
    "    return model_generator, model_discriminator\n",
    "\n",
    "def save_model_gan(path, model_generator, model_discriminator):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_model(os.path.join(path, 'generator'), model_generator)\n",
    "    save_model(os.path.join(path, 'discriminator'), model_discriminator)\n",
    "\n",
    "def save_plot_gan(path, model_generator, model_discriminator):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_plot(os.path.join(path, 'generator.png'), model_generator)\n",
    "    save_plot(os.path.join(path, 'discriminator.png'), model_discriminator)\n",
    "\n",
    "def save_summary_gan(path, model_generator, model_discriminator):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_summary(os.path.join(path, 'generator.txt'), model_generator)\n",
    "    save_summary(os.path.join(path, 'discriminator.txt'), model_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f1c99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e65ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'v1-pix2pix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4036b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_saved   = os.path.join(config.dir_model_saved  , model_name)\n",
    "path_model_plot    = os.path.join(config.dir_model_plot   , model_name)\n",
    "path_model_summary = os.path.join(config.dir_model_summary, model_name)\n",
    "\n",
    "path_progress_image = os.path.join(config.dir_progress_image, model_name)\n",
    "path_progress_loss  = os.path.join(config.dir_progress_loss , model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018b933",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcffe18",
   "metadata": {},
   "source": [
    "Initialize model from new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0756031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_generator     = create_model_generator(config.input_shape)\n",
    "# model_discriminator = create_model_discriminator(config.input_shape, config.output_shape)\n",
    "\n",
    "# save_model_gan(path_model_saved, model_generator, model_discriminator)\n",
    "# save_plot_gan(path_model_plot, model_generator, model_discriminator)\n",
    "# save_summary_gan(path_model_summary, model_generator, model_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e469ef",
   "metadata": {},
   "source": [
    "Initialize model from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e276e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully loaded model from ../Resources/model_saved/v1-pix2pix\\discriminator\n"
     ]
    }
   ],
   "source": [
    "model_generator, model_discriminator = load_model_gan(path_model_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e26ddb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c106b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gan = create_model_gan(model_generator, model_discriminator, config.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536272e",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1d958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress_image(model_generator, epochs):\n",
    "    path_sample = config.dir_progress_image_sample\n",
    "    path = os.path.join(config.dir_progress_image, model_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_names = os.listdir(path_sample)\n",
    "    images = np.array([cv2.imread(os.path.join(path_sample, file_name), 0) for file_name in file_names])\n",
    "    images = (np.array(images, 'float32') / 127.5 ) - 1\n",
    "    images = model_generator(images).numpy()\n",
    "    images = (images + 1) * 127.5\n",
    "    images = np.array(images, 'uint8')\n",
    "    for i, image in enumerate(images):\n",
    "        cv2.imwrite(os.path.join(path, '%s-%s'%(str(epochs).zfill(5), file_names[i])), image)\n",
    "    \n",
    "def log_progress_loss(loss_real, loss_fake, loss_gan, epochs):\n",
    "    path = os.path.join(config.dir_progress_loss, model_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(path, 'loss.txt'), 'a') as f:\n",
    "        f.write(\"%d %.4f %.4f %.4f\\n\"%(epochs, loss_real, loss_fake, loss_gan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f8b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model_generator, model_discriminator, model_gan, epochs, offset=0):\n",
    "    for e in range(epochs):\n",
    "        sum_loss_real = 0\n",
    "        sum_loss_fake = 0\n",
    "        sum_loss_gan = 0\n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        for index in range(dataset.__len__()):\n",
    "            print('Epochs: %d (iteration: %d/%d) [%.1f s]'%(e+1, index+1,dataset.__len__(), time.time()-t), end='\\r')\n",
    "            batch_grayscale, batch_color = dataset.__getitem__(index)\n",
    "            batch_fake = model_generator(batch_grayscale).numpy()\n",
    "            \n",
    "            y_real = np.ones((len(batch_grayscale), config.patch_shape[0], config.patch_shape[1]))\n",
    "            y_fake = np.zeros((len(batch_grayscale), config.patch_shape[0], config.patch_shape[1]))\n",
    "\n",
    "            loss_real = model_discriminator.train_on_batch([batch_grayscale, batch_color], y_real)\n",
    "            loss_fake = model_discriminator.train_on_batch([batch_grayscale, batch_fake], y_fake)\n",
    "            loss_gan, _, _ = model_gan.train_on_batch(batch_grayscale, [batch_color, y_real])\n",
    "            \n",
    "            sum_loss_real += loss_real\n",
    "            sum_loss_fake += loss_fake\n",
    "            sum_loss_gan += loss_gan\n",
    "            \n",
    "        print()\n",
    "        print('Loss Real: %.4f'%sum_loss_real)\n",
    "        print('Loss Fake: %.4f'%sum_loss_fake)\n",
    "        print('Loss GAN : %.4f'%sum_loss_gan)\n",
    "        print()\n",
    "        \n",
    "        log_progress_image(model_generator, e+1+offset)\n",
    "        log_progress_loss(sum_loss_real, sum_loss_fake, sum_loss_gan, e+1+offset)\n",
    "            \n",
    "        dataset.shuffle()\n",
    "        \n",
    "        save_model_gan(path_model_saved, model_generator, model_discriminator)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b74a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 (iteration: 1000/1000) [969.9 s]\n",
      "Loss Real: 212.1023\n",
      "Loss Fake: 610.1647\n",
      "Loss GAN : 8762.1548\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 2 (iteration: 1000/1000) [752.9 s]\n",
      "Loss Real: 141.7094\n",
      "Loss Fake: 684.8001\n",
      "Loss GAN : 7490.9162\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 3 (iteration: 1000/1000) [751.4 s]\n",
      "Loss Real: 131.6225\n",
      "Loss Fake: 696.3476\n",
      "Loss GAN : 7193.1104\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 4 (iteration: 1000/1000) [749.8 s]\n",
      "Loss Real: 120.1866\n",
      "Loss Fake: 701.5717\n",
      "Loss GAN : 7050.9931\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 5 (iteration: 1000/1000) [750.2 s]\n",
      "Loss Real: 115.9747\n",
      "Loss Fake: 694.4555\n",
      "Loss GAN : 6895.3165\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 6 (iteration: 1000/1000) [750.1 s]\n",
      "Loss Real: 113.0533\n",
      "Loss Fake: 686.8547\n",
      "Loss GAN : 6738.5824\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 7 (iteration: 1000/1000) [750.1 s]\n",
      "Loss Real: 112.3816\n",
      "Loss Fake: 682.4507\n",
      "Loss GAN : 6708.5605\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 8 (iteration: 1000/1000) [749.4 s]\n",
      "Loss Real: 112.0726\n",
      "Loss Fake: 678.0831\n",
      "Loss GAN : 6601.5026\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 9 (iteration: 1000/1000) [749.5 s]\n",
      "Loss Real: 114.7226\n",
      "Loss Fake: 683.8888\n",
      "Loss GAN : 6570.8922\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 10 (iteration: 1000/1000) [750.0 s]\n",
      "Loss Real: 121.7289\n",
      "Loss Fake: 693.4907\n",
      "Loss GAN : 6494.2608\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 11 (iteration: 1000/1000) [748.2 s]\n",
      "Loss Real: 121.4792\n",
      "Loss Fake: 700.5403\n",
      "Loss GAN : 6460.6153\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 12 (iteration: 1000/1000) [745.8 s]\n",
      "Loss Real: 123.4179\n",
      "Loss Fake: 706.1881\n",
      "Loss GAN : 6408.2098\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 13 (iteration: 1000/1000) [747.7 s]\n",
      "Loss Real: 121.0634\n",
      "Loss Fake: 705.2562\n",
      "Loss GAN : 6367.6850\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 14 (iteration: 1000/1000) [746.9 s]\n",
      "Loss Real: 118.4380\n",
      "Loss Fake: 703.7039\n",
      "Loss GAN : 6321.1555\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 15 (iteration: 1000/1000) [747.0 s]\n",
      "Loss Real: 117.7689\n",
      "Loss Fake: 701.9785\n",
      "Loss GAN : 6287.5294\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 16 (iteration: 1000/1000) [748.2 s]\n",
      "Loss Real: 114.9721\n",
      "Loss Fake: 698.3847\n",
      "Loss GAN : 6230.7211\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 17 (iteration: 1000/1000) [747.6 s]\n",
      "Loss Real: 113.1584\n",
      "Loss Fake: 694.6921\n",
      "Loss GAN : 6212.1641\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 18 (iteration: 1000/1000) [746.8 s]\n",
      "Loss Real: 113.8368\n",
      "Loss Fake: 688.6153\n",
      "Loss GAN : 6210.7352\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 19 (iteration: 1000/1000) [745.3 s]\n",
      "Loss Real: 113.1390\n",
      "Loss Fake: 687.4970\n",
      "Loss GAN : 6168.7535\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 20 (iteration: 1000/1000) [746.5 s]\n",
      "Loss Real: 115.8885\n",
      "Loss Fake: 685.9578\n",
      "Loss GAN : 6164.8650\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 21 (iteration: 1000/1000) [745.7 s]\n",
      "Loss Real: 113.2733\n",
      "Loss Fake: 684.1007\n",
      "Loss GAN : 6140.6198\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 22 (iteration: 1000/1000) [746.7 s]\n",
      "Loss Real: 115.7062\n",
      "Loss Fake: 685.6448\n",
      "Loss GAN : 6092.4602\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 23 (iteration: 1000/1000) [746.2 s]\n",
      "Loss Real: 114.6708\n",
      "Loss Fake: 683.5188\n",
      "Loss GAN : 6091.2978\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 24 (iteration: 1000/1000) [746.9 s]\n",
      "Loss Real: 114.9468\n",
      "Loss Fake: 683.8586\n",
      "Loss GAN : 6090.0166\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 25 (iteration: 1000/1000) [747.6 s]\n",
      "Loss Real: 116.1945\n",
      "Loss Fake: 685.3782\n",
      "Loss GAN : 6072.5913\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 26 (iteration: 1000/1000) [746.1 s]\n",
      "Loss Real: 114.3698\n",
      "Loss Fake: 686.2339\n",
      "Loss GAN : 6038.8896\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 27 (iteration: 1000/1000) [746.7 s]\n",
      "Loss Real: 116.2328\n",
      "Loss Fake: 682.5148\n",
      "Loss GAN : 6025.2393\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 28 (iteration: 1000/1000) [747.0 s]\n",
      "Loss Real: 116.2906\n",
      "Loss Fake: 685.9662\n",
      "Loss GAN : 5987.0338\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 29 (iteration: 1000/1000) [746.5 s]\n",
      "Loss Real: 115.3172\n",
      "Loss Fake: 683.0935\n",
      "Loss GAN : 6008.5567\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n",
      "Epochs: 30 (iteration: 1000/1000) [746.2 s]\n",
      "Loss Real: 113.6322\n",
      "Loss Fake: 682.2823\n",
      "Loss GAN : 5984.7715\n",
      "\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\generator\n",
      "Successfully saved model to ../Resources/model_saved/v1-pix2pix\\discriminator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, model_generator, model_discriminator, model_gan, 30, offset=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
